
<!-- TOC -->

- [第1章 统计学习方法概论](#1)
    - [1.2 监督学习 supervised learning](#12--supervised-learning)
        - [1.2.1 基本概念](#121)
            - [输入空间、特征空间、输出空间](#)
            - [联合概率分布](#)
            - [假设空间](#)
        - [1.2.2 问题的形式化](#122)
    - [1.3 统计学习三要素](#13)
        - [1.3.1 模型](#131)
            - [假设空间可以定义为决策函数的集合](#)
            - [假设空间可以定义为条件概率的集合](#)
        - [1.3.2 策略](#132)
            - [损失函数、风险函数](#)
            - [经验风险最小化、结构风险最小化](#)

<!-- /TOC -->

# 第1章 统计学习方法概论

## 1.2  监督学习  supervised learning

### 1.2.1 基本概念

#### 输入空间、特征空间、输出空间

每个具体的输入是一个实例(instance)，通常有特征向量(feature vector)表示。所有特征向量所在的空间即为特征空间(feature space)。

本书中 向量均为**列向量**。  
输入实例$x$的特征向量记作$\vec{x}=(x^{(1)},x^{(2)},...,x^{(i)},...,x^{(n)})^{T}$，  
而$x^{(i)}$和$x_{i}$不同，即$\vec{x_{i}}=(x^{(1)}_{i},x^{(2)}_{i},...,x^{(n)}_{i})^{T}$。  

训练集由输入(或特征向量)与输出对组成 $T=\{(x_{1},y_{1}),(x_{2},y_{2}),...(x_{N},y_{N})\}$, 输入输出对又称为样本(sample)。  

| 输入变量 | 输出变量 | 预测任务 |
| :------: | :------: | :------: |
| 连续变量 | 连续变量 | 回归问题 |
|          | 离散变量 | 分类问题 |
| 变量序列 | 变量序列 | 标注问题 |

#### 联合概率分布

监督学习假设输入输出随机变量最受联合概率分布$P=(X,Y)$,训练数据与测试数据看作是依此分布[iid](../../Statistical_theory/Independent_and_identically_distributed_random_variables.md)产生的。

#### 假设空间

 模型$\in$假设空间(hypothesis space)$=$即*输入空间到输出空间的映射的集合*，假设空间确定意味着学习范围的确定。

监督学习的模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数（decisionfunction）$Y=f(X)$表示，随具体学习方法而定。对具体的输入进行相应的输出预测时，写作$P(y|x)$或$Y=f(x)$。

### 1.2.2 问题的形式化

![image](图1.1_监督学习问题.png)

在学习过程中得到一个模型，表示为条件概率分布$\hat{P}(Y|X)$或决策函数$Y=\hat{f}(X)$,其描述了输入输出r.v.的映射关系。  
在预测过程中，预测系统对于给定 的测试样本集中的输入$x_{N+1}$，由模型$y_{N+1}=\arg \underset{y_{N+1}}{\max} \hat{P}(y_{N+1}|x_{N+1})$或$y_{N+1}=\hat{f}(x_{N+1})$给出相应的输出$y_{N+1}$。

## 1.3 统计学习三要素

**<center>方法=模型+策略+算法</center>**

### 1.3.1 模型

模型的<u>**假设空间**</u>(hypothesis sapce)包含所有可能的条件概率分布或决策函数。  
$X$和$Y$是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$的变量或随机变量。
参数向量$\theta$取值于$n$维欧式空间$\mathbf{R}^{n}$,称为<u>**参数空间**</u>(parameter sapce)。

#### 假设空间可以定义为决策函数的集合 

$$\mathcal{F}=\{ f|Y=f(X)\}$$   <div align="right" id = "fomula1_1">$(1.1)$</div>  

这时$\mathcal{F}$通常是由一个参数向量决定的函数族
$$\mathcal{F}=\{ f|Y=f_{\theta}(X),\theta\in\mathbf{R}^{n}\}$$ <div align="right" id = "fomula1_2">$(1.2)$</div>  

#### 假设空间可以定义为条件概率的集合

$$\mathcal{F}=\{ P|P(Y|X)\}$$   <div align="right" id = "fomula1_3">$(1.3)$</div>  
这时$\mathcal{F}$通常是由一个参数向量决定的条件概率分布族
$$\mathcal{F}=\{ P|P_{\theta}(Y|X),\theta\in\mathbf{R}^{n}\}$$   <div align="right" id = "fomula1_4">$(1.4)$</div>  

### 1.3.2 策略

统计学习的目标在于从假设空间中选取最优模型。

#### 损失函数、风险函数

损失函数$L(Y,f(X))$是一个非负实值函数，常用的函数有以下几种：

1. 0-1损失函数  0-1 loss function


$$L(Y,f(X)) = \begin{cases}
   1, &Y\not = f(X)  \\
   0, &Y     = f(X)
\end{cases}$$   <div align="right" id = "fomula1_5">$(1.5)$</div>  

2. 平方损失函数 quadratic loss function

$$L(Y,f(X))=(Y-f(X))^2$$    <div align="right" id = "fomula1_6">$(1.6)$</div>

3. 绝对损失函数 absolute loss function

$$L(Y,f(X))=|Y-f(X)|$$  <div align="right" id = "fomula1_7">$(1.7)$</div>

4. 对数似然损失函数 loglikelihood loss function

$$L(Y,P|P(Y|X))=-\log P(Y|X)$$  <div align="right" id = "fomula1_8">$(1.8)$</div>

由于$(X,Y)$是遵循联合分布$P(X,Y)$的随机变量，所以损失函数的期望即是<u>**风险函数**</u> risk function 或 <u>**期望损失**</u> expect loss
$$R_{exp}(f)=E_{p}[L(Y,f(X))]=\int_{\mathcal{X} \times \mathcal{Y}} L(y,f(x)) P(x,y) \text{d}x \text{d}y$$  <div align="right" id = "fomula1_9">$(1.9)$</div>

模型$f(X)$关于训练数据集$T=\{(x_{1},y_{1}),(x_{2},y_{2}),...(x_{N},y_{N})\}$的平均损失成为<u>**经验风险**</u>或<u>**经验损失**</u>，即
$$R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_{i},f(x_{i}))$$    <div align="right" id = "fomula1_10">$(1.10)$</div>

期望风险$R_{exp}(f)$是模型关于联合分布的期望损失  
经验风险$R_{emp}(f)$是模型关于训练样本的平均损失  
根据大数定律，当样本容量$N\rightarrow \infty$时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$。
但是现实中训练样本数目有限，所以要对经验风险进行一定的矫正，这就关系到监督学习的两个基本策略:<u>**经验风险最小化**</u>和<u>**结构风险最小化**</u>

#### 经验风险最小化、结构风险最小化
 
在假设空间、损失函数、训练数据集确定的情况下，[经验风险函数式][1.10]就可以得到确定，经验风险最小化(empirical risk minimization, ERM)的策略认为，经验风险最小的模型是最优的模型。据此 按照经验风险最小化求最优模型就是求解最优化问题
$$\underset{f \in \mathcal{F}}{\min} R_{emp}(f)=\underset{f \in \mathcal{F}}{\min} \frac{1}{N} \sum^{N}_{i=1} L(y_{i},f(x_{i}))$$   <div align="right" id = "fomula1_11">$(1.11)$</div>  

当模型是条件概率分布，损失函数是对数损失函数时，**经验风险最小化就等价于极大似然估计**。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生后面将要叙述的<u>**过拟合**</u>(over-fitting)现象。

结构风险最小化（structural risk minimization，SRM）是为了防止过拟合而提出来的策略。结构风险最小化等价于<u>**正则化**</u>（regularization）。结构风险在经验风险上加上表示模型复杂度的**正则化项**（regularizer）或**罚项**（penaltyterm）。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是

$$\begin{aligned}
   R_{srm}(f) &=R_{emp}(f) &+\lambda J(f) \\
   &=\frac{1}{N}\sum^{N}_{i=1}L(y_{i},f(x_{i}))&+\lambda J(f) 
\end{aligned}$$ <div align="right" id = "fomula1_12">$(1.12)$</div>  

其中$J(f)$为模型的复杂度，是定义在假设空间上的泛函。模型$f$越复杂，复杂度$J(f)$就越大；反之，模型f越简单，复杂度J(f)就越小。也就是说，复杂度表示了对复杂模型的惩罚。$\lambda ≥ 0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。


- [ ] 贝叶斯估计中的最大后验概率估计（ maximum posterior probability estimation， MAP）






































[1.10]:#fomula1_10 "(1.10)"